{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Value Iteration algorithm applied to Taxi in OpenAI GYM\n",
    "\n",
    "This weeks coding challenge is to use the OpenAI Gym library to build a simple value iteration algorithm for any of the available environments. In this Notebook we're going to implement the algorithm to solve the **Taxi** game from Gym. More information <a href=\"https://gym.openai.com/envs/Taxi-v1/\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 How to play the game?\n",
    "\n",
    "It's a simple game. At least for humans :) \n",
    "\n",
    "Let's see it. You have a Taxi. In a little \"city\". You also have the direction of a passenger and a destination. Of course, you have to go to the position of your passenger, pick them and go to the destination **in the shortest path**\n",
    "\n",
    "<img src=\"src/TheTaxiGame.png\">\n",
    "\n",
    "Hope that with the image it's all clear :D\n",
    "\n",
    "### 1.1.1 Theory of the game\n",
    "\n",
    "This task was introduced in [Dietterich2000] to illustrate some **issues in hierarchical reinforcement learning**. There are **4 locations** (labeled by different letters) and your job is to pick up the **passenger at one location** and **drop** him off **in another**. You receive **+20 points for a successful dropoff**, and **lose 1 point for every timestep** it takes. There is also a **10 point penalty for illegal pick-up and drop-off actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The RL problem in theory [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "\n",
    "- An agent: The Taxi\n",
    "- An environment: The \"city\"\n",
    "- A reward: The points\n",
    "- A set of states: The positions of the taxi and the passenger and destination\n",
    "- A set of actions: At every time step, the directions the taxi can go\n",
    "\n",
    "We are going to apply the **value iteration** algorithm so, first of all it's important to know what is a **value function**\n",
    "\n",
    "#### Value function:\n",
    "\n",
    "Often denoted V(s), represent **how good** is a state for an agent to be in. The value function depends on the policy by which the agent picks actions to perform. So, if the agent uses a given policy ùõë to select actions, the corresponding value function is given by: \n",
    "\n",
    "<img src=\"src/value_function_1.png\" width=\"500px\">\n",
    "\n",
    "Among all possible value-functions, there exist an optimal value function that has higher value than other functions for all states.\n",
    "\n",
    "<img src=\"src/optimal_value_function.png\" width=\"500px\">\n",
    "\n",
    "The optimal policy ùõë* is the policy that corresponds to optimal value function.\n",
    "\n",
    "<img src=\"src/optimal_policy.png\" width=\"500px\">\n",
    "\n",
    "#### Q function:\n",
    "\n",
    "Now we introduce another function which is the **state-action pair Q function**.\n",
    "\n",
    "<img src=\"src/q_func_simple.png\" width=\"200px\">\n",
    "\n",
    "Q(s, a) is an indication for how good it is for an agent to pick action a while being in state s.\n",
    "\n",
    "To find the optimal --> Q\\*(s,a)\n",
    "\n",
    "The Q\\*(s, a) is equal to the summation of immediate reward after performing action a while in state s and the discounted expected future reward after transition to a next state s'.\n",
    "\n",
    "<img src=\"src/q_optimal.png\" width=\"500px\">\n",
    "\n",
    "This is the **Bellman equation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration **computes the optimal state value function** by **iteratively improving the estimate of V(s)**.\n",
    "\n",
    "The algorithm **initialize V(s)** to arbitrary **random** values. It repeatedly **updates the Q(s, a) and V(s) values until they convergs**. Value iteration is **guranteed to converge to the optimal values**.\n",
    "\n",
    "<img src=\"src/value_iteration_pseudo.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the game technically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-19 17:45:16,461] Making new env: Taxi-v2\n"
     ]
    }
   ],
   "source": [
    "#Get the game environment\n",
    "env_name = 'Taxi-v2'\n",
    "env_wrapped = gym.make(env_name)\n",
    "env = env_wrapped.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the possible **states** and **actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space A -> len(A): 6\n",
      "States space S -> len(S): 500\n"
     ]
    }
   ],
   "source": [
    "print('Action space A -> len(A): %d' % env.action_space.n)\n",
    "print('States space S -> len(S): %d' % env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a set of **6 possible actions**<br>\n",
    "and a set of **500 possible states**\n",
    "\n",
    "Not bad for start :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reinforcement Learning for Taxi implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The value function will represent a VALUE for each STATE, initialize it to zeros.\n",
    "\n",
    "- We define a number of max iterations _(max_iterations)_ to converge and a minimum increasing value _(eps)_ to consider not improving.\n",
    "\n",
    "**In the training loop:**\n",
    "\n",
    "1. Store the last value function\n",
    "2. For each STATE _s_ and For each ACTION _a_ \n",
    "\n",
    "    2.1 Iterate the NEXT STATES you can go from that determined state-action pair (_s_,_a_)<br>\n",
    "    2.2 Store the reward from those NEXT STATES according to the formula: **probability * (reward + PreviousValueFunction[NEXT\\_STATE])**<br>\n",
    "\n",
    "3. **Sum** all those rewards from NEXT STATES<br>\n",
    "4. **Choose the max** reward of _q(s,a)_ pairs and put it on the actual value function for STATE s\n",
    "\n",
    "5. If the actual value functions hasn't improved more than the improving threshold, convergence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0):\n",
    "\n",
    "    '''\n",
    "    Value-iteration algorithm\n",
    "    '''\n",
    "    #The value function will represent a VALUE for each STATE, initialize it to zeros.\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    max_iterations = 10000\n",
    "    display_freq = max_iterations // 10\n",
    "    eps = 1e-20\n",
    "    lastDif = float('inf')\n",
    "    \n",
    "    #Start training loop\n",
    "    print('Starting training loop...')\n",
    "    for i in range(max_iterations):\n",
    "        if(i % display_freq == 0):\n",
    "            print('25 first elements of actual value function. An array of number of possible states (%d) elements:\\n%s'\n",
    "                  %(env.observation_space.n,v[0:25]))\n",
    "            \n",
    "        prev_v = np.copy(v) #Store the last value function\n",
    "        for s in range(env.observation_space.n):#For each STATE s\n",
    "            q_sa = []\n",
    "            for a in range(env.action_space.n): #For each ACTION a\n",
    "                next_states_rewards = []\n",
    "                for next_sr in env.P[s][a]: #Iterate the states you can go from determined state-action pair (s,a)\n",
    "                    p, s_, r, _ = next_sr #(probability, next_state, reward, done) of the states you can go from (s,a)\n",
    "                    next_states_rewards.append((p*(r + prev_v[s_]))) #Apply the formula to get the rewards\n",
    "                q_sa.append(np.sum(next_states_rewards)) #Store the sum of rewards for each pair (s,a)\n",
    "            v[s] = max(q_sa) #Choose the max reward of (s,a) pairs and put it on the actual value function for STATE s\n",
    "            \n",
    "        if(np.abs(np.abs(np.sum(prev_v - v)) - lastDif) < eps): #Check convergence\n",
    "            print('Value-iteration converged at iteration %d' % (i+1))\n",
    "            break\n",
    "        lastDif = np.abs(np.sum(prev_v - v))\n",
    "    return v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "25 first elements of actual value function. An array of number of possible states (500) elements:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n",
      "Value-iteration converged at iteration 19\n"
     ]
    }
   ],
   "source": [
    "optimal_value_function = value_iteration(env=env,gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the optimal value functions looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function shape: (500,)\n",
      "First 25 values of the value functions:\n",
      "[ 359.  233.  275.  212.  107.  233.   65.  128.  191.  107.  275.  128.\n",
      "   65.  107.   65.  212.  380.  254.  296.  233.  338.  212.  254.  191.\n",
      "  128.]\n"
     ]
    }
   ],
   "source": [
    "print('Value function shape: %s\\nFirst 25 values of the value functions:\\n%s' \n",
    "      % (optimal_value_function.shape,optimal_value_function[0:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a real number for each possible state, seems right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Extract policy\n",
    "\n",
    "Because we want our agent to **do something** and our agents **behaves according** to the **policy**, the **policy** could be **random actions** or a **value function**...\n",
    "\n",
    "Now that we have a (optimal) function value we can **extract** the associated (optimal) policy.\n",
    "\n",
    "**How:**\n",
    "Just get the argument that maximize (argmax) the value function for each possible state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(v, gamma = 1.0):\n",
    "    '''\n",
    "    Extract the policy given a value-function\n",
    "    '''\n",
    "    \n",
    "    policy = np.zeros(env.observation_space.n) #Policy : array of 0s with as many elements as possible states\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n) # q_sa: array of 0s with as many elements as possible actions\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]: #Iterate the states you can go from determined state-action pair\n",
    "                #next_sr is a tuple of (probability, next_state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract the policy for the optimal value function that we've just computed with the value iteration algorithm\n",
    "optimal_policy = extract_policy(v=optimal_value_function, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the optimal value functions looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function shape: (500,)\n",
      "First 25 values of the value functions:\n",
      "[ 4.  4.  4.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.\n",
      "  0.  0.  3.  3.  3.  3.  0.]\n"
     ]
    }
   ],
   "source": [
    "print('Value function shape: %s\\nFirst 25 values of the value functions:\\n%s' \n",
    "      % (optimal_value_function.shape,optimal_policy[0:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a number in [0,5] for each state. Guess it!\n",
    "\n",
    "Yes, it's the (optimal) action to take in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluate policy\n",
    "\n",
    "In fact it's playing the game.\n",
    "\n",
    "1. Get in a starting state (reset the environment)\n",
    "2. Take the (optimal) action according to the state --> Ask the policy!\n",
    "3. Do it\n",
    "4. If it's ended. Stop.\n",
    "\n",
    "\\*find each element of this numbered list in the code by the index ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma = 1.0, verbose=True,render = False):\n",
    "    '''\n",
    "    Evaluates a policy by using it to run an episode and finding it's total reward\n",
    "    \n",
    "    returns: \n",
    "    total reward: real value of the total reward received by the agent under policy\n",
    "    '''\n",
    "    \n",
    "    state = env.reset() #1. Reset the environment, starting state.\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "       \n",
    "        if render:\n",
    "            env.render(mode = 'human')\n",
    "            \n",
    "        action = int(policy[state]) #2. Ask the policy for the optimal action!\n",
    "        state, reward, done, info = env.step(action) #3. Do it!\n",
    "        if verbose:\n",
    "            print('State: %d - Reward: %d - Done: %d' %(state, reward, done))\n",
    "            \n",
    "        total_reward = total_reward + (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        \n",
    "        if done: #4. If it's ended stop\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "State: 201 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "State: 101 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "State: 1 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "State: 17 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "State: 117 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "State: 137 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "State: 157 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "State: 57 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: |\u001b[42m_\u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "State: 77 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "State: 97 - Reward: -1 - Done: 0\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "State: 97 - Reward: 20 - Done: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env,optimal_policy,verbose=True,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it a lot of times for evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    '''\n",
    "    Evaluates a policy by running it n times\n",
    "    \n",
    "    returns:\n",
    "    average total reward\n",
    "    '''\n",
    "    scores = [run_episode(env,policy,gamma,False,False) for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have got 10.20 average points in 10 games\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "points = evaluate_policy(env,optimal_policy,gamma=1.0,n=n)\n",
    "print('We have got %.2f average points in %d games' %(points,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Formal evaluation\n",
    "\n",
    "According to the <a href=\"https://gym.openai.com/envs/Taxi-v1/\">official documentation</a>, **Taxi-v1 defines \"solving\" as getting average reward of 9.7 over 100 consecutive trials.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have got 8.50 average points in 100 games\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "points = evaluate_policy(env,optimal_policy,gamma=1.0,n=n)\n",
    "print('We have got %.2f average points in %d games' %(points,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all...\n",
    "\n",
    "<img src=\"src/playing_with_neural_nets.jpg\" width=\"250px\">\n",
    "\n",
    "Seriously, check it in the render mode, the taxi driver is awesome! Always takes the shortest path! Don't know what else to do :O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "- Tweak _gamma_ values and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Taxi - OpenAI Gym - https://gym.openai.com/envs/Taxi-v1/ <br>\n",
    "[2] An overview of MAXQ hierarchical reinforcement learning - https://link.springer.com/book/10.1007/3-540-44914-0#page=38<br>\n",
    "[3] Deep Reinforcement Learning Demysitifed (Episode 2)‚Ää‚Äî‚ÄäPolicy Iteration, Value Iteration and Q-learning - https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
